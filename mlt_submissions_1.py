# -*- coding: utf-8 -*-
"""MLT_submissions_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MGh8vAjt0aIpy4r79-_pNX1X26kT5MUc

# **Proyek Pertama : Predictive Analytics**


Panduan Project : [Dicoding](https://www.dicoding.com/academies/319/tutorials/17052)

Prediksi kelas Pumkin Seeds :
- Proyek ini menghasilkan model yang dapat memprediksi suatu biji labu termasuk pada kelas `Çerçevelik` atau `Ürgüp Sivrisi`.
- Model ini bekerja melalui banyak prediktor yang berpengaruh seperti Area, Roundess, Aspect Ratio dan masih banyak lagi.

- Dataset yang digunakan dalam model ini berasal dari ==> [Pumpkin Seeds Dataset](https://www.kaggle.com/datasets/muratkokludataset/pumpkin-seeds-dataset/)

- Proyek ini dikerjakan dalam rangka menyelesaikan kelas [Machine Learning Terapan](https://www.dicoding.com/academies/319)

Nama : Moch. Avin

Asal : Surabaya, Jawa Timur

# Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, confusion_matrix

"""# Load data"""

df=pd.read_excel("/content/Pumpkin_Seeds_Dataset.xlsx",sheet_name='Pumpkin_Seeds_Dataset')
df.head()

"""# Eksplorasi data

## Deskripsi Variabel - Analisis Univariate

- `Area`: Luas area

- `Perimeter`: Keliling
- `Major_Axis_Length`: Panjang sumbu utama
- `Minor_Axis_Length`: Panjang sumbu minor
- `Convex_Area`: Luas konveks
- `Equiv_Diameter`: Diameter
- `Eccentricity`: Eksentrisitas
- `Solidity`: Soliditas
- `Extent`: Ekstensi
- `Roundness`: Kebulatan
- `Aspect_Ration`: Rasio aspek
- `Compactness`: Kepadatan
- `Class`:   Kelas `Çerçevelik` dipresentasikan sebagai 0, atau Kelas `Ürgüp Sivrisi` dipresentasikan sebagai 1
"""

df.info()
df.describe()

columns_to_analyze = ['Area', 'Convex_Area', 'Equiv_Diameter', 'Solidity', 'Perimeter',
                      'Major_Axis_Length', 'Eccentricity', 'Aspect_Ration', 'Extent',
                      'Minor_Axis_Length', 'Roundness', 'Compactness']

# Mengatur ukuran kanvas
plt.figure(figsize=(20, 30))

# Membuat plot untuk setiap kolom
for i, column in enumerate(columns_to_analyze, 1):
    plt.subplot(6, 2a, i)  # 4 baris, 3 kolom, nomor plot ke-i
    sns.histplot(data=df, x=column, kde=True)
    plt.title(f"{column} Distribution", fontsize=15)

plt.tight_layout()
plt.show()

"""## Mengecek Missing Value"""

df.isnull().sum()

"""> Tidak terdapat nilai yang kosong (*missing value*) sehingga kita dapat ke tahap selanjutnya.

## Mengecek keseimbangan Class
"""

sns.countplot(df["Class"])
plt.show()

"""## Encoding Class

Mengubah Çerçevelik menjadi 0 dan Ürgüp Sivrisi menjadi 1

"""

from sklearn.preprocessing import OrdinalEncoder
ordinalEncoder=OrdinalEncoder()
df["Class"]=ordinalEncoder.fit_transform(df[["Class"]])

df.head()

"""## Mengecek dan Menangani Outliers

Mengecek outlier dengan boxplot
"""

y_columns = df.columns[df.columns != 'Class']

# Membuat satu kanvas dan beberapa sumbu
fig, axes = plt.subplots(nrows=len(y_columns)//2, ncols=2, figsize=(12, len(y_columns)*2))

# Menampilkan boxplot untuk setiap kolom y
for i, y_column in enumerate(y_columns):
    row = i // 2
    col = i % 2
    sns.boxplot(x='Class', y=y_column, data=df, ax=axes[row, col])
    axes[row, col].set_title(f'Boxplot of {y_column}')
    axes[row, col].set_xlabel('Class')
    axes[row, col].set_ylabel(y_column)

# Menyesuaikan tata letak
plt.tight_layout()
plt.show()

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)

IQR = Q3 - Q1

df_clean = df[~((df<(Q1-1.5*IQR)) | (df>(Q3+1.5*IQR))).any(axis=1)]

# Cek ukuran dataset setelah kita drop outliers
df_clean.shape

"""> Setelah membuang nilai outlier data kita berubah dari 2500 baris dan 13 kolom menjadi 2285 baris dan 13 kolom."""

y_columns = df_clean.columns[df_clean.columns != 'Class']

# Membuat satu kanvas dan beberapa sumbu
fig, axes = plt.subplots(nrows=len(y_columns)//2, ncols=2, figsize=(12, len(y_columns)*2))

# Menampilkan boxplot untuk setiap kolom y
for i, y_column in enumerate(y_columns):
    row = i // 2
    col = i % 2
    sns.boxplot(x='Class', y=y_column, data=df_clean, ax=axes[row, col])
    axes[row, col].set_title(f'Boxplot of {y_column}')
    axes[row, col].set_xlabel('Class')
    axes[row, col].set_ylabel(y_column)

# Menyesuaikan tata letak
plt.tight_layout()
plt.show()

"""## Mengecek korelasi antar variabel

Mengecek menggunakan headmap dan plot antar variabel
"""

# Correlation
corr = df.corr()

# Figure
plt.figure(figsize=(10,8))
sns.heatmap(corr, annot=True)
plt.show()

plt.figure(figsize=(15,20))
sns.pairplot(data=df)
plt.show()

"""Melalui Correlation Matrix dan Pairplot di atas dapat disimpulkan :

* `Area`, `Convex_Area`, `Equiv_Diameter`, `Solidity` berkolerasi lemah terhadap `Class` secara positif
* `Perimeter`, `Major_Axis_Length` berkolerasi sedang terhadap `Class` secara positif
* `Eccentricity`, `Aspect_Ration` berkolerasi kuat terhadap `Class` secara positif
* `Extent` berkolerasi lemah terhadap `Class` secara negatif
* `Minor_Axis_Length` berkolerasi sedang terhadap `Class` secara negatif
* `Roundness`, `Compactness` berkolerasi kuat terhadap `Class` secara negatif

# Data Preparation

> Terlihat di heatmap maupun di plot bahwa `Area`, `Convex_Area`, `Equiv_Diameter` memiliki Value yang hampir sama, maka dipilih salah satu saja yaitu `Area`
"""

# drop kolom Convex_Area dan Equiv_Diameter karena sangat mirip dengan kolom Area
columns_to_drop = ['Convex_Area', 'Equiv_Diameter']
df_clean = df_clean.drop(columns_to_drop, axis=1)

df_clean.head()

"""## Split data train dan data test

Membagi dataset menjadi data latih (train) dan data uji (test) merupakan hal yang harus kita lakukan sebelum membuat model. Karena banyak data yang ada sekitar 2000 an maka 80% data train dan 20% data test cukup ideal.
"""

X = df_clean.drop(['Class'], axis = 1)
y = df_clean['Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)

print(f"Total # of sample in whole dataset: {len(X)}")
print(f"Total # of sample in train dataset: {len(X_train)}")
print(f"Total # of sample in test dataset: {len(X_test)}")

"""## Standarisasi

Standarisasi adalah proses penting dalam persiapan data untuk algoritma machine learning. Tujuannya adalah untuk menghasilkan data dengan skala relatif serupa atau mendekati distribusi normal, sehingga algoritma machine learning dapat memiliki performa yang lebih baik dan konvergen lebih cepat.

Dalam proses standarisasi, kita tidak akan menggunakan teknik seperti one-hot-encoding yang biasanya digunakan untuk fitur kategorikal. Sebaliknya, kita akan menggunakan teknik StandarScaler yang tersedia dalam library Scikit-learn. Ini akan membantu dalam mengubah fitur-fitur numerik sehingga memiliki mean 0 dan varians 1.

Melalui standarisasi, kita membuat data lebih mudah diolah oleh algoritma machine learning, memungkinkan mereka untuk bekerja lebih efisien dan menghasilkan hasil yang lebih baik.
"""

scaler = StandardScaler()

# Menghitung mean dan standar deviasi dari data pelatihan
scaler.fit(X_train)

# Standarisasi data pelatihan
X_train_scaled = scaler.transform(X_train)

# Standarisasi data pengujian dengan menggunakan parameter yang sama
X_test_scaled = scaler.transform(X_test)

"""# Model Development

kita akan menggunakan KNN, RandomForest, dan AdaBoostClassifier

## KNN
KNN memanfaatkan konsep "kesamaan fitur" untuk memprediksi nilai setiap data baru berdasarkan kemiripannya dengan titik data dalam set pelatihan. Dengan menggunakan algoritma ini, kita menentukan prediksi berdasarkan k-neighbors terdekat dari data baru. KNN adalah pendekatan sederhana namun efektif dalam berbagai kasus, menawarkan kemudahan interpretasi dan penerapan yang luas dalam pemodelan data terstruktur.
"""

# Menentukan parameter k (jumlah data terdekat)
knn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(X_train_scaled, y_train)

pred = knn.predict(X_train_scaled)
knn_f1 = f1_score(y_train, pred)
print("KNN - F1 Score train : {:.4}%\n".format(knn_f1*100))

"""## Random Forest

Random Forest adalah algoritma pembelajaran mesin yang kuat dan serbaguna yang memanfaatkan teknik ensemble dengan cara membangun sejumlah besar pohon keputusan selama pelatihan. Setiap pohon keputusan dalam Random Forest diberi bobot dan keputusan akhir diambil berdasarkan mayoritas suara dari semua pohon. Pendekatan ensemble ini membantu mengurangi overfitting dan meningkatkan ketahanan model terhadap noise dalam data, membuatnya cocok untuk berbagai macam masalah klasifikasi dan regresi.
"""

rfc = RandomForestClassifier(max_depth=5, max_features=20, random_state=56)
rfc.fit(X_train_scaled, y_train)

pred = rfc.predict(X_train_scaled)

rfc_f1 = f1_score(y_train, pred)
print("RF - F1 Score train : {:.4}%\n".format(rfc_f1*100))

"""`max_depth`: Mengatur kedalaman maksimum dari setiap pohon dalam Random Forest. Semakin tinggi nilainya, semakin kompleks modelnya, tetapi terlalu tinggi dapat menyebabkan overfitting.

`max_features`: Menentukan jumlah fitur yang akan dipertimbangkan oleh setiap pohon saat mencari pemisahan yang optimal. Ini membantu mengurangi overfitting dengan membatasi variasi antar pohon.

`random_state`: Mengontrol keacakan dalam pembangunan model. Dengan menetapkan nilai yang sama, Anda dapat mendapatkan hasil yang konsisten dari setiap eksekusi model. Ideal untuk tujuan reproduktibilitas dan perbandingan model.

## Boosting Algorithm

Boosting adalah teknik di machine learning di mana model lemah digunakan secara berulang dan ditingkatkan untuk membuat model yang lebih kuat. Model lemah ini fokus pada kesalahan sebelumnya dan diberi bobot lebih besar untuk memperbaikinya. Dengan cara ini, Boosting meningkatkan performa model secara bertahap, menjadikannya efektif untuk menangani masalah klasifikasi atau regresi yang kompleks. Di sini digunakan AdaBoost.
"""

from sklearn.ensemble import AdaBoostClassifier

boosting = AdaBoostClassifier(learning_rate = 0.0001, random_state = 42)
boosting.fit(X_train_scaled, y_train)

pred = boosting.predict(X_train_scaled)
boosting_f1 = f1_score(y_train, pred)
print("Boosting - F1 Score train : {:.4}%\n".format(boosting_f1*100))

"""`learning_rate`: Mengatur seberapa besar setiap model lemah dalam ensemble "belajar" dari kesalahan sebelumnya. Semakin kecil nilai learning_rate, semakin lambat pembelajaran model.

`random_state`: Mengontrol keacakan dalam pembangunan model. Dengan menetapkan nilai yang sama, Anda dapat memastikan hasil yang konsisten dari satu eksekusi ke eksekusi lainnya.

# Evaluasi model

Evaluasi model adalah proses penting dalam pengembangan model machine learning yang bertujuan untuk mengukur kinerja dan keakuratan model terhadap data latihan dan data uji. Dalam konteks kode yang diberikan, kita sedang mengevaluasi beberapa algoritma machine learning (`KNN`, `Random Forest Classifier`, dan `Boosting`) dengan menggunakan metrik evaluasi `F1 Score` pada kedua set data, yaitu data latihan dan data uji. Hasil evaluasi model disimpan dalam sebuah dataframe bernama "models", yang berisi skor F1 untuk setiap algoritma pada kedua set data train dan test.

`F1 Score` merupakan metrik evaluasi yang digunakan dalam klasifikasi untuk mengukur keseimbangan antara presisi (`precision`) dan `recall` dari suatu model. Dengan menggunakan rata-rata harmonis dari kedua metrik tersebut, `F1 Score` memberikan gambaran yang baik tentang seberapa baik model dapat memprediksi kelas positif secara tepat dan seberapa banyak kelas positif yang dapat diidentifikasi secara keseluruhan. Hal ini membuat `F1 Score` berguna terutama dalam kasus di mana distribusi kelas tidak seimbang atau ketika kita ingin memperhitungkan trade-off antara `Precision` dan `Recall` dalam evaluasi model klasifikasi.
"""

# Buat variabel models yang isinya adalah dataframe nilai models data train dan test pada masing-masing algoritma
models = pd.DataFrame(columns=['train', 'test'], index = ['KNN', 'rfc', 'Boosting'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'rfc': rfc, 'Boosting': boosting}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
  models.loc[name, 'train'] = f1_score(y_true=y_train, y_pred=model.predict(X_train_scaled))
  models.loc[name, 'test'] = f1_score(y_true=y_test, y_pred=model.predict(X_test_scaled))

models

"""Untuk memudahkan, dibuat visualisasi perbandingan peforma ketiga model tersebut"""

# Memplot Hasil Metric
fig, ax = plt.subplots()
models.sort_values(by="test", ascending=False).plot(kind='bar', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Menguji 10 sample dengan menggunakan ketiga model tersebut"""

# Buat 10 sample prediksi dengan y_true
prediksi = X_test_scaled[0:10].copy()
pred_dict = {'y_true': y_test[0:10]}

for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(0)

pred_df = pd.DataFrame(pred_dict)
print(pred_df)

"""# Conclusion


Dalam proyek ini, penulis telah menyelesaikan serangkaian tahapan yang penting. Penulis memulai dengan memahami domain proyek dan kasus bisnis yang ingin diselesaikan. Setelah itu, penulis menyelami data yang tersedia, menyiapkan dataset, melakukan proses pemodelan, dan mengevaluasi hasilnya. Penulis berhasil menyelesaikan tantangan bisnis yang dihadapi dan mencapai tujuan dengan membangun 3 model yang berbeda. Dari ketiga model tersebut, penulis memilih model `Random Forest` sebagai solusi terbaik untuk masalah yang dihadapi.
"""